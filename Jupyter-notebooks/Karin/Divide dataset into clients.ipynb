{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29ed779",
   "metadata": {},
   "source": [
    "**Deviding dataset into imbalanced clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27144ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_path = r\"C:\\Users\\karin\\Documents\\Federated_learning\\cleaned\\Training\"\n",
    "print(os.listdir(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e99c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client datasets created: all clients have all classes, fully imbalanced.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "# Base preprocessed dataset\n",
    "base_path = r\"C:\\Users\\karin\\Documents\\Federated_learning\\cleaned\\Training\"\n",
    "client_base = r\"C:\\Users\\karin\\Documents\\Federated_learning\\clients\"\n",
    "\n",
    "# Clients / hospitals\n",
    "clients = [\"client1\", \"client2\", \"client3\", \"client4\"]\n",
    "\n",
    "# Remove old client folders if they exist\n",
    "if os.path.exists(client_base):\n",
    "    shutil.rmtree(client_base)\n",
    "os.makedirs(client_base)\n",
    "\n",
    "# Class names\n",
    "classes = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Read all images per class\n",
    "all_images = {}\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(base_path, cls)\n",
    "    images = [os.path.join(cls_path, f) for f in os.listdir(cls_path)]\n",
    "    all_images[cls] = images\n",
    "\n",
    "# Total images per client (simulate hospital sizes)\n",
    "total_images_per_client = {\n",
    "    \"client1\": 1000,\n",
    "    \"client2\": 400,\n",
    "    \"client3\": 200,\n",
    "    \"client4\": 50\n",
    "}\n",
    "\n",
    "# Fully imbalanced class fractions per client (sum not 1, intentional)\n",
    "class_fractions = {\n",
    "    \"client1\": {\"glioma\": 0.7, \"meningioma\": 0.2, \"pituitary\": 0.08, \"notumor\": 0.02},\n",
    "    \"client2\": {\"glioma\": 0.05, \"meningioma\": 0.1, \"pituitary\": 0.7, \"notumor\": 0.15},\n",
    "    \"client3\": {\"glioma\": 0.1, \"meningioma\": 0.05, \"pituitary\": 0.1, \"notumor\": 0.75},\n",
    "    \"client4\": {\"glioma\": 0.4, \"meningioma\": 0.3, \"pituitary\": 0.2, \"notumor\": 0.1},\n",
    "}\n",
    "\n",
    "# Create client datasets\n",
    "for client in clients:\n",
    "    client_path = os.path.join(client_base, client, \"Training\")\n",
    "    os.makedirs(client_path, exist_ok=True)\n",
    "    total_imgs = total_images_per_client[client]\n",
    "\n",
    "    for cls, frac in class_fractions[client].items():\n",
    "        cls_path = os.path.join(client_path, cls)\n",
    "        os.makedirs(cls_path, exist_ok=True)\n",
    "\n",
    "        # Number of images for this class\n",
    "        num_images = max(1, math.floor(total_imgs * frac))\n",
    "        # Randomly select images without removing from original\n",
    "        selected = random.sample(all_images[cls], min(num_images, len(all_images[cls])))\n",
    "\n",
    "        for img_path in selected:\n",
    "            shutil.copy(img_path, cls_path)\n",
    "\n",
    "print(\"Client datasets created: all clients have all classes, fully imbalanced.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e5a39",
   "metadata": {},
   "source": [
    "**Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Base path where client datasets are stored\n",
    "client_base = r\"C:\\Users\\karin\\Documents\\Federated_learning\\clients\"\n",
    "\n",
    "# Define augmentation transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(20),                     # rotation range 20 degrees\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.2, 0.2),                         # width and height shift 20%\n",
    "        shear=20,                                     # shear 20 degrees\n",
    "        fill=0                                        # fill empty pixels with black\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.2)),  # zoom range ~20%\n",
    "    transforms.RandomHorizontalFlip(p=0.5),           # horizontal flip\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For validation/testing (no augmentation, only resize and tensor)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create DataLoaders for each client\n",
    "client_loaders = {}\n",
    "batch_size = 16\n",
    "\n",
    "for client in os.listdir(client_base):\n",
    "    client_path = os.path.join(client_base, client, \"Training\")\n",
    "    if not os.path.isdir(client_path):\n",
    "        continue\n",
    "\n",
    "    dataset = datasets.ImageFolder(client_path, transform=train_transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    client_loaders[client] = loader\n",
    "\n",
    "print(\"DataLoaders with augmentation created for clients:\", list(client_loaders.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6df7b",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pretrained ResNet (without final classification layer)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "modules = list(resnet.children())[:-1]  # remove final FC layer\n",
    "feature_extractor = nn.Sequential(*modules).to(device)\n",
    "feature_extractor.eval()  # set to eval mode\n",
    "\n",
    "# Example: extract features for a batch\n",
    "for client, loader in client_loaders.items():\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(images)  # shape: [batch_size, 512, 1, 1]\n",
    "            features = features.view(features.size(0), -1)  # flatten to [batch_size, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32e7f9",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_features_dict = {}\n",
    "client_features_dict = {}\n",
    "\n",
    "for client, loader in client_loaders.items():\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(images)           # [batch_size, 512, 1, 1]\n",
    "            features = features.view(features.size(0), -1) # flatten to [batch_size, 512]\n",
    "        \n",
    "        all_features.append(features.cpu())  # move to CPU and store\n",
    "        all_labels.append(labels)            # labels are already CPU tensors\n",
    "\n",
    "    # Stack all batches to one tensor per client\n",
    "    client_features_dict[client] = {\n",
    "        \"features\": torch.cat(all_features, dim=0),  # [total_images, 512]\n",
    "        \"labels\": torch.cat(all_labels, dim=0)       # [total_images]\n",
    "    }\n",
    "\n",
    "print(\"Feature extraction complete. Example shapes:\")\n",
    "for client, data in client_features_dict.items():\n",
    "    print(client, data[\"features\"].shape, data[\"labels\"].shape)\n",
    "\n",
    "for client, loader in client_loaders.items():\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(images)           # [batch_size, 512, 1, 1]\n",
    "            features = features.view(features.size(0), -1) # flatten to [batch_size, 512]\n",
    "        \n",
    "        all_features.append(features.cpu())  # move to CPU and store\n",
    "        all_labels.append(labels)            # labels are already CPU tensors\n",
    "\n",
    "    # Stack all batches to one tensor per client\n",
    "    client_features_dict[client] = {\n",
    "        \"features\": torch.cat(all_features, dim=0),  # [total_images, 512]\n",
    "        \"labels\": torch.cat(all_labels, dim=0)       # [total_images]\n",
    "    }\n",
    "\n",
    "print(\"Feature extraction complete. Example shapes:\")\n",
    "for client, data in client_features_dict.items():\n",
    "    print(client, data[\"features\"].shape, data[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea8393",
   "metadata": {},
   "source": [
    "**Feature Selection and Federated Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Determine fixed PCA dimension\n",
    "# -----------------------------\n",
    "min_samples = min([client_features_dict[c]['features'].shape[0] for c in client_features_dict])\n",
    "fixed_n_components = min(50, min_samples)  # adjust 50 or lower if some client has very few images\n",
    "print(\"Fixed PCA components:\", fixed_n_components)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Apply PCA per client\n",
    "# -----------------------------\n",
    "client_reduced_features = {}\n",
    "for client in client_features_dict:\n",
    "    X = client_features_dict[client]['features'].numpy()\n",
    "    y = client_features_dict[client]['labels'].numpy()\n",
    "    \n",
    "    pca = PCA(n_components=fixed_n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    client_reduced_features[client] = {\n",
    "        \"features\": X_reduced,\n",
    "        \"labels\": y\n",
    "    }\n",
    "    \n",
    "    print(client, \"reduced features shape:\", X_reduced.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define MLP\n",
    "# -----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=fixed_n_components, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: FedAvg function\n",
    "# -----------------------------\n",
    "def fed_avg(weights_list):\n",
    "    avg_weights = {}\n",
    "    for key in weights_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weights_list]) / len(weights_list)\n",
    "    return avg_weights\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Initialize global model\n",
    "# -----------------------------\n",
    "global_model = MLP(input_dim=fixed_n_components, num_classes=4)\n",
    "global_weights = global_model.state_dict()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Federated learning loop\n",
    "# -----------------------------\n",
    "num_rounds = 5\n",
    "local_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    local_weights = []\n",
    "    print(f\"--- Round {round+1} ---\")\n",
    "    \n",
    "    for client in client_reduced_features:\n",
    "        X = torch.tensor(client_reduced_features[client]['features'], dtype=torch.float32)\n",
    "        y = torch.tensor(client_reduced_features[client]['labels'], dtype=torch.long)\n",
    "        \n",
    "        # Initialize local model with global weights\n",
    "        model = MLP(input_dim=fixed_n_components, num_classes=4)\n",
    "        model.load_state_dict(copy.deepcopy(global_weights))\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Local training\n",
    "        for epoch in range(local_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        local_weights.append(copy.deepcopy(model.state_dict()))\n",
    "    \n",
    "    # FedAvg aggregation\n",
    "    global_weights = fed_avg(local_weights)\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "print(\"Federated training complete. Global model ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3652b12",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e326c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Evaluate local models (trained individually per client)\n",
    "# -----------------------------\n",
    "print(\"=== Local Model Evaluation ===\")\n",
    "local_model_performance = {}\n",
    "\n",
    "for client in client_reduced_features:\n",
    "    # Initialize a new MLP for this client\n",
    "    model = MLP(input_dim=fixed_n_components, num_classes=4).to(device)\n",
    "    \n",
    "    # Train locally (same as in federated loop)\n",
    "    X = torch.tensor(client_reduced_features[client]['features'], dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(client_reduced_features[client]['labels'], dtype=torch.long).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(5):  # local training epochs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss_val = criterion(outputs, y).item()\n",
    "        acc_val = (preds == y).float().mean().item()\n",
    "    \n",
    "    local_model_performance[client] = {\"loss\": loss_val, \"accuracy\": acc_val}\n",
    "    print(f\"{client} - Loss: {loss_val:.4f}, Accuracy: {acc_val:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Evaluate global federated model\n",
    "# -----------------------------\n",
    "print(\"\\n=== Global Federated Model Evaluation ===\")\n",
    "global_model.eval()\n",
    "\n",
    "for client in client_reduced_features:\n",
    "    X = torch.tensor(client_reduced_features[client]['features'], dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(client_reduced_features[client]['labels'], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = global_model(X)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss_val = criterion(outputs, y).item()\n",
    "        acc_val = (preds == y).float().mean().item()\n",
    "    \n",
    "    print(f\"{client} - Loss: {loss_val:.4f}, Accuracy: {acc_val:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Global evaluation across all clients combined\n",
    "# -----------------------------\n",
    "X_all = np.vstack([client_reduced_features[c]['features'] for c in client_reduced_features])\n",
    "y_all = np.hstack([client_reduced_features[c]['labels'] for c in client_reduced_features])\n",
    "\n",
    "X_all_t = torch.tensor(X_all, dtype=torch.float32).to(device)\n",
    "y_all_t = torch.tensor(y_all, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = global_model(X_all_t)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    loss_all = criterion(outputs, y_all_t).item()\n",
    "    acc_all = (preds == y_all_t).float().mean().item()\n",
    "\n",
    "print(f\"\\nGlobal Evaluation (all clients combined) - Loss: {loss_all:.4f}, Accuracy: {acc_all:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_all_t.cpu().numpy(), preds.cpu().numpy(),\n",
    "                            target_names=['glioma','meningioma','notumor','pituitary']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
